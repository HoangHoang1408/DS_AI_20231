{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "sns.set()\n",
    "rcParams[\"figure.figsize\"] = (20, 10)\n",
    "pd.options.display.max_columns = None\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "base_path = \"/Users/phamhoang1408/Desktop/20231/DS/ds_project/models/features\"\n",
    "with open(f\"{base_path}/cap_bac.json\", \"r\") as f:\n",
    "    cap_bac_feature_values = json.load(f)\n",
    "with open(f\"{base_path}/dia_diem_lam_viec.json\", \"r\") as f:\n",
    "    dia_diem_lam_viec_feature_values = json.load(f)\n",
    "with open(f\"{base_path}/hinh_thuc.json\", \"r\") as f:\n",
    "    hinh_thuc_feature_values = json.load(f)\n",
    "with open(f\"{base_path}/loai_hinh_hoat_dong.json\", \"r\") as f:\n",
    "    loai_hinh_hoat_dong_feature_values = json.load(f)\n",
    "with open(f\"{base_path}/nganh_nghe.json\", \"r\") as f:\n",
    "    nganh_nghe_feature_values = json.load(f)\n",
    "with open(f\"{base_path}/quy_mo_cong_ty.json\", \"r\") as f:\n",
    "    quy_mo_cong_ty_feature_values = json.load(f)\n",
    "with open(f\"{base_path}/ten_cong_ty.json\", \"r\") as f:\n",
    "    ten_cong_ty_feature_values = json.load(f)\n",
    "\n",
    "with open(f\"{base_path}/num_followers.json\", \"r\") as f:\n",
    "    num_followers_feature_values = json.load(f)\n",
    "\n",
    "vi_tri_viec_vectorizer = joblib.load(f\"{base_path}/vi_tri_viec_vectorizer.joblib\")\n",
    "\n",
    "\n",
    "def convert_raw_data_to_feature(raw_data):\n",
    "    def convert_text_unique_to_feature(feature_values, value):\n",
    "        vector = np.zeros(len(feature_values))\n",
    "        if value in feature_values:\n",
    "            vector[feature_values.index(value)] = 1\n",
    "        elif None in feature_values:\n",
    "            vector[-1] = 1\n",
    "        return vector\n",
    "\n",
    "    def convert_numeric_to_feature(ranges, value):\n",
    "        vector = np.zeros(len(ranges))\n",
    "        if value is None:\n",
    "            vector[-1] = 1\n",
    "        else:\n",
    "            for i, r in enumerate(ranges):\n",
    "                if r[0] <= value <= r[1]:\n",
    "                    vector[i] = 1\n",
    "                    break\n",
    "        return vector\n",
    "\n",
    "    def convert_tf_idf_to_feature(vectorizer, value):\n",
    "        return vectorizer.transform([value]).toarray()[0]\n",
    "\n",
    "    feature_vector = np.concatenate(\n",
    "        [\n",
    "            convert_text_unique_to_feature(cap_bac_feature_values, raw_data[\"cap_bac\"]),\n",
    "            convert_text_unique_to_feature(\n",
    "                dia_diem_lam_viec_feature_values, raw_data[\"dia_diem_lam_viec\"]\n",
    "            ),\n",
    "            convert_text_unique_to_feature(\n",
    "                hinh_thuc_feature_values, raw_data[\"hinh_thuc\"]\n",
    "            ),\n",
    "            convert_text_unique_to_feature(\n",
    "                loai_hinh_hoat_dong_feature_values, raw_data[\"loai_hinh_hoat_dong\"]\n",
    "            ),\n",
    "            convert_text_unique_to_feature(\n",
    "                nganh_nghe_feature_values, raw_data[\"nganh_nghe\"]\n",
    "            ),\n",
    "            convert_text_unique_to_feature(\n",
    "                quy_mo_cong_ty_feature_values, raw_data[\"quy_mo_cong_ty\"]\n",
    "            ),\n",
    "            convert_text_unique_to_feature(\n",
    "                ten_cong_ty_feature_values, raw_data[\"ten_cong_ty\"]\n",
    "            ),\n",
    "            convert_tf_idf_to_feature(vi_tri_viec_vectorizer, raw_data[\"vi_tri_viec\"]),\n",
    "            convert_numeric_to_feature(\n",
    "                num_followers_feature_values, raw_data[\"num_followers\"]\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def salary_mapper(x):\n",
    "    try:\n",
    "        t = None\n",
    "        if \"tr vnd\" in x[\"luong\"].lower():\n",
    "            t = \"tr vnd\"\n",
    "        elif \"usd\" in x[\"luong\"].lower():\n",
    "            t = \"usd\"\n",
    "        min_luong, max_luong = None, None\n",
    "        if \"-\" in x[\"luong\"]:\n",
    "            a, b = x[\"luong\"].split(\"-\")\n",
    "            temp1 = a.strip().split(\" \")[0].replace(\",\", \"\").replace(\".\", \"\")\n",
    "            temp2 = b.strip().split(\" \")[0].replace(\",\", \"\").replace(\".\", \"\")\n",
    "            min_luong = int(temp1)\n",
    "            max_luong = int(temp2)\n",
    "        elif \"Trên\" in x[\"luong\"]:\n",
    "            min_luong = int(x[\"luong\"].split(\" \")[1].replace(\",\", \"\").replace(\".\", \"\"))\n",
    "        elif \"Lên đến\" in x[\"luong\"]:\n",
    "            max_luong = int(x[\"luong\"].split(\" \")[2].replace(\",\", \"\").replace(\".\", \"\"))\n",
    "        if t == \"usd\":\n",
    "            if min_luong:\n",
    "                min_luong = min_luong * 23 / 1000\n",
    "            if max_luong:\n",
    "                max_luong = max_luong * 23 / 1000\n",
    "        return {\n",
    "            \"min_luong\": min_luong,\n",
    "            \"max_luong\": max_luong,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "\n",
    "def min_filter(x):\n",
    "    return x[\"min_luong\"] is not None and x[\"min_luong\"] <= 200\n",
    "\n",
    "\n",
    "def min_range_mapper(x):\n",
    "    ranges = [\n",
    "        (0, 5),\n",
    "        (5, 10),\n",
    "        (10, 15),\n",
    "        (15, 20),\n",
    "        (20, 25),\n",
    "        (25, 30),\n",
    "        (30, 50),\n",
    "        (50, 75),\n",
    "        (75, 100),\n",
    "        (100, 999999),\n",
    "    ]\n",
    "    for i, r in enumerate(ranges):\n",
    "        if r[0] <= x[\"min_luong\"] <= r[1]:\n",
    "            return {\n",
    "                \"min_luong_range\": i,\n",
    "            }\n",
    "\n",
    "\n",
    "def max_filter(x):\n",
    "    return x[\"max_luong\"] is not None and x[\"max_luong\"] <= 200\n",
    "\n",
    "\n",
    "def max_range_mapper(x):\n",
    "    ranges = [\n",
    "        (0, 10),\n",
    "        (10, 20),\n",
    "        (20, 30),\n",
    "        (30, 50),\n",
    "        (50, 75),\n",
    "        (75, 100),\n",
    "        (100, 999999),\n",
    "    ]\n",
    "    for i, r in enumerate(ranges):\n",
    "        if r[0] <= x[\"max_luong\"] <= r[1]:\n",
    "            return {\n",
    "                \"max_luong_range\": i,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_json(\"../crawl/final/final_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['vi_tri_viec', 'ten_cong_ty', 'dia_diem_lam_viec', 'ngay_cap_nhat', 'nganh_nghe', 'hinh_thuc', 'luong', 'cap_bac', 'het_han_nop', 'dia_chi_cong_ty', 'loai_hinh_hoat_dong', 'quy_mo_cong_ty', 'num_followers', 'min_exp', 'max_exp'],\n",
       "    num_rows: 7184\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a72068457b04ce58cb3ddc28e05a979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3992 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Load data and filter out invalid data\n",
    "ds = ds.filter(lambda x: x[\"luong\"] not in [\"Thoả thuận\", \"Thương lượng\", \"Cạnh tranh\"])\n",
    "ds = ds.map(salary_mapper, remove_columns=[\"luong\"])\n",
    "ds_min = ds.filter(min_filter).map(min_range_mapper)\n",
    "ds_max = ds.filter(max_filter).map(max_range_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['vi_tri_viec', 'ten_cong_ty', 'dia_diem_lam_viec', 'ngay_cap_nhat', 'nganh_nghe', 'hinh_thuc', 'cap_bac', 'het_han_nop', 'dia_chi_cong_ty', 'loai_hinh_hoat_dong', 'quy_mo_cong_ty', 'num_followers', 'min_exp', 'max_exp', 'min_luong', 'max_luong', 'min_luong_range'],\n",
       "     num_rows: 4185\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['vi_tri_viec', 'ten_cong_ty', 'dia_diem_lam_viec', 'ngay_cap_nhat', 'nganh_nghe', 'hinh_thuc', 'cap_bac', 'het_han_nop', 'dia_chi_cong_ty', 'loai_hinh_hoat_dong', 'quy_mo_cong_ty', 'num_followers', 'min_exp', 'max_exp', 'min_luong', 'max_luong', 'max_luong_range'],\n",
       "     num_rows: 3992\n",
       " }))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_min, ds_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719bfecc725147c99713ddf71549a774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3992 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_min = ds_min.map(\n",
    "    lambda x: {\n",
    "        \"feature_vector\": convert_raw_data_to_feature(x),\n",
    "        \"min_luong\": x[\"min_luong\"],\n",
    "        \"max_luong\": x[\"max_luong\"],\n",
    "    }\n",
    ").select_columns([\"feature_vector\", \"min_luong\", \"min_luong_range\"])\n",
    "\n",
    "ds_max = ds_max.map(\n",
    "    lambda x: {\n",
    "        \"feature_vector\": convert_raw_data_to_feature(x),\n",
    "        \"min_luong\": x[\"min_luong\"],\n",
    "        \"max_luong\": x[\"max_luong\"],\n",
    "    }\n",
    ").select_columns([\"feature_vector\", \"max_luong\", \"max_luong_range\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_min_train = ds_min.train_test_split(train_size=0.9, seed=42)[\"train\"]\n",
    "ds_min_test = ds_min.train_test_split(train_size=0.9, seed=42)[\"test\"]\n",
    "X_train_min = ds_min_train[\"feature_vector\"]\n",
    "y_train_min = ds_min_train[\"min_luong_range\"]\n",
    "X_test_min = ds_min_test[\"feature_vector\"]\n",
    "y_test_min = ds_min_test[\"min_luong_range\"]\n",
    "\n",
    "ds_max_train = ds_max.train_test_split(train_size=0.9, seed=42)[\"train\"]\n",
    "ds_max_test = ds_max.train_test_split(train_size=0.9, seed=42)[\"test\"]\n",
    "X_train_max = ds_max_train[\"feature_vector\"]\n",
    "y_train_max = ds_max_train[\"max_luong_range\"]\n",
    "X_test_max = ds_max_test[\"feature_vector\"]\n",
    "y_test_max = ds_max_test[\"max_luong_range\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min score: 0.5656324582338902\n",
      "Max score: 0.575\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_min = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_min.fit(X_train_min, y_train_min)\n",
    "rf_max = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_max.fit(X_train_max, y_train_max)\n",
    "print(\"Min score:\",rf_min.score(X_test_min, y_test_min))\n",
    "print(\"Max score:\",rf_max.score(X_test_max, y_test_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min score: 0.5990453460620525\n",
      "Max score: 0.595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/models/lgmb_max.joblib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model_min = LGBMClassifier()\n",
    "model_min.fit(X_train_min, y_train_min)\n",
    "print(\"Min score:\",model_min.score(X_test_min, y_test_min))\n",
    "\n",
    "model_max = LGBMClassifier()\n",
    "model_max.fit(X_train_max, y_train_max)\n",
    "print(\"Max score:\",model_max.score(X_test_max, y_test_max))\n",
    "\n",
    "# save model\n",
    "joblib.dump(model_min, \"../models/models/lgmb_min.joblib\")\n",
    "joblib.dump(model_max, \"../models/models/lgmb_max.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min score: 0.5918854415274463\n",
      "Max score: 0.5925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/models/xgb_max.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model_min = XGBClassifier()\n",
    "model_min.fit(X_train_min, y_train_min)\n",
    "print(\"Min score:\",model_min.score(X_test_min, y_test_min))\n",
    "\n",
    "model_max = XGBClassifier()\n",
    "model_max.fit(X_train_max, y_train_max)\n",
    "print(\"Max score:\",model_max.score(X_test_max, y_test_max))\n",
    "\n",
    "# save model\n",
    "joblib.dump(model_min, \"../models/models/xgb_min.joblib\")\n",
    "joblib.dump(model_max, \"../models/models/xgb_max.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
